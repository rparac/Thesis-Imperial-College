@inproceedings{RefWorks:RefID:56-broda2016iterative,
	author={K. B. Broda and M. Law and A. Russo},
	year={Jul 25, 2016},
	title={Iterative Learning of Answer Set Programs with Context Dependent Examples},
	publisher={Cambridge University Press (CUP): STM Journals},
	abstract={In recent years, several frameworks and systems have been proposed that extend Inductive Logic Programming (ILP) to the Answer Set Programming (ASP) paradigm. In ILP, examples must all be explained by a hypothesis together with a given background knowledge. In existing systems, the background knowledge is the same for all examples; however, examples may be context-dependent. This means that some examples should be explained in the context of some information, whereas others should be explained in different contexts. In this paper, we capture this notion and present a context-dependent extension of the Learning from Ordered Answer Sets framework. In this extension, contexts can be used to further structure the background knowledge. We then propose a new iterative algorithm, ILASP2i, which exploits this feature to scale up the existing ILASP2 system to learning tasks with large numbers of examples. We demonstrate the gain in scalability by applying both algorithms to various learning tasks. Our results show that, compared to ILASP2, the newly proposed ILASP2i system can be two orders of magnitude faster and use two orders of magnitude less memory, whilst preserving the same average accuracy},
	isbn={1475-3081},
	url={http://hdl.handle.net/10044/1/52844},
	doi={10.1017/S1471068416000351}
}
@article{RefWorks:RefID:55-law2018inductive,
	author={Mark Law and Alessandra Russo and Krysia Broda},
	year={2018},
	month={Aug 25,},
	title={Inductive Learning of Answer Set Programs from Noisy Examples},
	abstract={In recent years, non-monotonic Inductive Logic Programming has received
growing interest. Specifically, several new learning frameworks and algorithms
have been introduced for learning under the answer set semantics, allowing the
learning of common-sense knowledge involving defaults and exceptions, which are
essential aspects of human reasoning. In this paper, we present a
noise-tolerant generalisation of the learning from answer sets framework. We
evaluate our ILASP3 system, both on synthetic and on real datasets, represented
in the new framework. In particular, we show that on many of the datasets
ILASP3 achieves a higher accuracy than other ILP systems that have previously
been applied to the datasets, including a recently proposed differentiable
learning framework.},
	url={https://arxiv.org/abs/1808.08441}
}
@misc{RefWorks:RefID:54-ilasp,
	title={ILASP Releases},
	url={https://github.com/ilaspltd/ILASP-releases}
}
@misc{RefWorks:RefID:53-mit,
	title={MIT License},
	url={https://opensource.org/licenses/MIT}
}
@inproceedings{RefWorks:RefID:52-assari2014video,
	author={Shayan Modiri Assari and Amir Roshan Zamir and Mubarak Shah},
	year={Jun 2014},
	title={Video Classification Using Semantic Concept Co-occurrences},
	publisher={IEEE},
	pages={2529-2536},
	abstract={We address the problem of classifying complex videos based on their content. A typical approach to this problem is performing the classification using semantic attributes, commonly termed concepts, which occur in the video. In this paper, we propose a contextual approach to video classification based on Generalized Maximum Clique Problem (GMCP) which uses the co-occurrence of concepts as the context model. To be more specific, we propose to represent a class based on the co-occurrence of its concepts and classify a video based on matching its semantic co-occurrence pattern to each class representation. We perform the matching using GMCP which finds the strongest clique of co-occurring concepts in a video. We argue that, in principal, the co-occurrence of concepts yields a richer representation of a video compared to most of the current approaches. Additionally, we propose a novel optimal solution to GMCP based on Mixed Binary Integer Programming (MBIP). The evaluations show our approach, which opens new opportunities for further research in this direction, outperforms several well established video classification methods.},
	isbn={1063-6919},
	url={https://ieeexplore.ieee.org/document/6909720},
	doi={10.1109/CVPR.2014.324}
}
@article{RefWorks:RefID:51-jianping2007incorporating,
	author={Jianping Fan and Hangzai Luo and Yuli Gao and R. Jain},
	year={2007},
	month={Aug},
	title={Incorporating Concept Ontology for Hierarchical Video Classification, Annotation, and Visualization},
	journal={IEEE transactions on multimedia},
	volume={9},
	number={5},
	pages={939-957},
	abstract={Most existing content-based video retrieval (CBVR) systems are now amenable to support automatic low-level feature extraction, but they still have limited effectiveness from a user's perspective because of the semantic gap. Automatic video concept detection via semantic classification is one promising solution to bridge the semantic gap. To speed up SVM video classifier training in high-dimensional heterogeneous feature space, a novel multimodal boosting algorithm is proposed by incorporating feature hierarchy and boosting to reduce both the training cost and the size of training samples significantly. To avoid the inter-level error transmission problem, a novel hierarchical boosting scheme is proposed by incorporating concept ontology and multitask learning to boost hierarchical video classifier training through exploiting the strong correlations between the video concepts. To bridge the semantic gap between the available video concepts and the users' real needs, a novel hyperbolic visualization framework is seamlessly incorporated to enable intuitive query specification and evaluation by acquainting the users with a good global view of large-scale video collections. Our experiments in one specific domain of surgery education videos have also provided very convincing results.},
	isbn={1520-9210},
	url={https://ieeexplore.ieee.org/document/4276710},
	doi={10.1109/TMM.2007.900143}
}
@inproceedings{RefWorks:RefID:50-fan2004semantic,
	author={Jianping Fan and Hangzai Luo and Jing Xiao and Lide Wu},
	year={Jun 7, 2004},
	title={Semantic video classification and feature subset selection under context and concept uncertainty},
	series={JCDL '04},
	publisher={ACM},
	address={New York NY},
	pages={192-201},
	abstract={As large collections of videos become one key component of digital libraries, there is an urgent need of semantic video classification and feature subset selection to enable more effective video database organization and retrieval. However, most existing techniques for classifier training require a large number of labeled samples to learn correctly and suffer from the problems of context and concept uncertainty when only a limited number of labeled samples are available. To address the problems of context and concept uncertainty, we have proposed a novel framework to achieve incremental classifier training by integrating a limited number of labeled samples with a large number of unlabeled samples. Specifically, the contributions of this paper include: (a) Using the salient objects to achieve a middle-level understanding of video contents and enhance the quality of features on discriminating among different semantic video concepts; (b) Modeling the semantic video concepts by using the finite mixture models to approximate the class distributions of the relevant salient objects; (c) Developing an adaptive EM algorithm to integratethe unlabeled samples to enable incremental classifier training and address the problem of context uncertainty; (d) Proposing a cost-sensitive video classification technique to address the problem of concept uncertainty over time; (e) Supporting automatic video annotation via semantic classification Our experimental results in a certain domain of medical education videos have also been provided a convincing proof of our conclusions.},
	url={http://dl.acm.org/citation.cfm?id=996395},
	doi={10.1145/996350.996395}
}
@inproceedings{RefWorks:RefID:49-szegedy2016rethinking,
	author={Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jon Shlens and Zbigniew Wojna},
	year={Jun 2016},
	title={Rethinking the Inception Architecture for Computer Vision},
	publisher={IEEE},
	pages={2818-2826},
	abstract={Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2\% top-1 and 5:6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5\% top-5 error and 17:3\% top-1 error on the validation set and 3:6\% top-5 error on the official test set.},
	url={https://ieeexplore.ieee.org/document/7780677},
	doi={10.1109/CVPR.2016.308}
}
@inbook{RefWorks:RefID:47-law2014inductive,
	author={Mark Law and Alessandra Russo and Krysia Broda},
	year={2014},
	month={Jan 1,},
	title={Inductive Learning of Answer Set Programs},
	series={Logics in Artificial Intelligence},
	publisher={Springer International Publishing},
	address={Cham},
	pages={311-325},
	abstract={Existing work on Inductive Logic Programming (ILP) has focused mainly on the learning of definite programs or normal logic programs. In this paper, we aim to push the computational boundary to a wider class of programs: Answer Set Programs. We propose a new paradigm for ILP that integrates existing notions of brave and cautious semantics within a unifying learning framework whose inductive solutions are Answer Set Programs and examples are partial interpretations We present an algorithm that is sound and complete with respect to our new notion of inductive solutions. We demonstrate its applicability by discussing a prototype implementation, called ILASP (Inductive Learning of Answer Set Programs), and evaluate its use in the context of planning. In particular, we show how ILASP can be used to learn agent’s knowledge about the environment. Solutions of the learned ASP program provide plans for the agent to travel through the given environment.},
	isbn={331911557X},
	url={http://link.springer.com/10.1007/978-3-319-11558-0_22},
	doi={10.1007/978-3-319-11558-0_22}
}
@misc{RefWorks:RefID:46-corapi2010inductive,
	author = 	 {Domenico Corapi and Alessandra Russo and Emil Lupu},
	year = 	 {2010},
	month = 	 {Jan 1,},
	title = 	 {Inductive Logic Programming as Abductive Search},
	url = 	 {https://explore.openaire.eu/search/publication?articleId=datacite____::7c2d6219485a0794937043e4a9de60b9},
	doi={10.4230/lipics.iclp.2010.54}
}
@inbook{RefWorks:RefID:45-muggletontoplog:,
	author={Stephen H. Muggleton and José Carlos Almeida Santos and Alireza Tamaddoni-Nezhad},
	title={TopLog: ILP Using a Logic Program Declarative Bias},
	series={Logic Programming},
	publisher={Springer Berlin Heidelberg},
	address={Berlin, Heidelberg},
	volume={5366},
	pages={687-692},
	abstract={This paper introduces a new Inductive Logic Programming (ILP) framework called Top Directed Hypothesis Derivation (TDHD). In this framework each hypothesised clause must be derivable from a given logic program called top theory (⊤). The top theory can be viewed as a declarative bias which defines the hypothesis space. This replaces the metalogical mode statements which are used in many ILP systems. Firstly we present a theoretical framework for TDHD and show that standard SLD derivation can be used to efficiently derive hypotheses from ⊤. Secondly, we present a prototype implementation of TDHD within a new ILP system called TopLog. Thirdly, we show that the accuracy and efficiency of TopLog, on several benchmark datasets, is competitive with a state of the art ILP system like Aleph.},
	isbn={9783540899815},
	url={http://link.springer.com/10.1007/978-3-540-89982-2_58},
	doi={10.1007/978-3-540-89982-2_58}
}
@inbook{RefWorks:RefID:44-ray2003hybrid,
	author={Oliver Ray and Krysia Broda and Alessandra Russo},
	year={2003},
	title={Hybrid Abductive Inductive Learning: A Generalisation of Progol},
	series={Inductive Logic Programming},
	publisher={Springer Berlin Heidelberg},
	address={Berlin, Heidelberg},
	pages={311-328},
	abstract={The learning system Progol5 and the underlying inference method of Bottom Generalisation are firmly established within Inductive Logic Programming (ILP). But despite their success, it is known that Bottom Generalisation, and therefore Progol5, are restricted to finding hypotheses that lie within the semantics of Plotkin’s relative subsumption. This paper exposes a previously unknown incompleteness of Progol5 with respect to Bottom Generalisation, and proposes a new approach, called Hybrid Abductive Inductive Learning, that integrates the ILP principles of Progol5 with Abductive Logic Programming (ALP). A proof procedure is proposed, called HAIL, that not only overcomes this newly discovered incompleteness, but further generalises Progol5 by computing multiple clauses in response to a single seed example and deriving hypotheses outside Plotkin’s relative subsumption. A semantics is presented, called Kernel Generalisation, which extends that of Bottom Generalisation and includes the hypotheses constructed by HAIL.},
	isbn={3540201440},
	url={http://link.springer.com/10.1007/978-3-540-39917-9_21},
	doi={10.1007/978-3-540-39917-9_21}
}
@inbook{RefWorks:RefID:43-muggleton2000theory,
	author={Stephen H. Muggleton and Christopher H. Bryant},
	year={2000},
	month={Aug 11,},
	title={Theory Completion Using Inverse Entailment},
	series={Inductive Logic Programming},
	publisher={Springer Berlin Heidelberg},
	address={Berlin, Heidelberg},
	pages={130-146},
	abstract={The main real-world applications of Inductive Logic Programming (ILP) to date involve the “Observation Predicate Learning” (OPL) assumption, in which both the examples and hypotheses define the same predicate. However, in both scientific discovery and language learning potential applications exist in which OPL does not hold. OPL is ingrained within the theory and performance testing of Machine Learning. A general ILP technique called “Theory Completion using Inverse Entailment” (TCIE) is introduced which is applicable to non-OPL applications. TCIE is based on inverse entailment and is closely allied to abductive inference. The implementation of TCIE within Progol5.0 is described. The implementation uses contra-positives in a similar way to Stickel’s Prolog Technology Theorem Prover. Progol5.0 is tested on two different data-sets. The first dataset involves a grammar which translates numbers to their representation in English. The second dataset involves hypothesising the function of unknown genes within a network of metabolic pathways. On both datasets near complete recovery of performance is achieved after relearning when randomly chosen portions of background knowledge are removed. Progol5.0’s running times for experiments in this paper were typically under 6 seconds on a standard laptop PC.},
	isbn={9783540677956},
	url={http://link.springer.com/10.1007/3-540-44960-4_8},
	doi={10.1007/3-540-44960-4_8}
}
@article{RefWorks:RefID:42-muggleton1991inductive,
	author={Stephen Muggleton},
	year={1991},
	month={Feb},
	title={Inductive Logic Programming},
	journal={New generation computing},
	volume={8},
	number={4},
	pages={295-318},
	isbn={0288-3635},
	doi={10.1007/BF03037089}
}
@article{RefWorks:RefID:41-gao2021clip2tv:,
	author={Zijian Gao and Jingyu Liu and Sheng Chen and Dedan Chang and Hao Zhang and Jinwei Yuan},
	year={2021},
	month={Nov 10,},
	title={CLIP2TV: An Empirical Study on Transformer-based Methods for Video-Text Retrieval},
	abstract={Modern video-text retrieval frameworks basically consist of three parts:
video encoder, text encoder and the similarity head. With the success on both
visual and textual representation learning, transformer based encoders and
fusion methods have also been adopted in the field of video-text retrieval. In
this report, we present CLIP2TV, aiming at exploring where the critical
elements lie in transformer based methods. To achieve this, We first revisit
some recent works on multi-modal learning, then introduce some techniques into
video-text retrieval, finally evaluate them through extensive experiments in
different configurations. Notably, CLIP2TV achieves 52.9@R1 on MSR-VTT dataset,
outperforming the previous SOTA result by 4.1\%.},
	url={https://arxiv.org/abs/2111.05610}
}
@inproceedings{RefWorks:RefID:40-jun2016msr-vtt:,
	author={Jun Xu and Tao Mei and Ting Yao and Yong Rui},
	year={Jun 2016},
	title={MSR-VTT: A Large Video Description Dataset for Bridging Video and Language},
	publisher={IEEE},
	pages={5288-5296},
	abstract={While there has been increasing interest in the task of describing video with natural language, current computer vision algorithms are still severely limited in terms of the variability and complexity of the videos and their associated language that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on specific fine-grained domains with limited videos and simple descriptions. While researchers have provided several benchmark datasets for image captioning, we are not aware of any large-scale video description dataset with comprehensive categories yet diverse video content. In this paper we present MSR-VTT (standing for "MSRVideo to Text") which is a new large-scale video benchmark for video understanding, especially the emerging task of translating video to text. This is achieved by collecting 257 popular queries from a commercial video search engine, with 118 videos for each query. In its current version, MSR-VTT provides 10K web video clips with 41.2 hours and 200K clip-sentence pairs in total, covering the most comprehensive categories and diverse visual content, and representing the largest dataset in terms of sentence and vocabulary. Each clip is annotated with about 20 natural sentences by 1,327 AMT workers. We present a detailed analysis of MSR-VTT in comparison to a complete set of existing datasets, together with a summarization of different state-of-the-art video-to-text approaches. We also provide an extensive evaluation of these approaches on this dataset, showing that the hybrid Recurrent Neural Networkbased approach, which combines single-frame and motion representations with soft-attention pooling strategy, yields the best generalization capability on MSR-VTT.},
	url={https://ieeexplore.ieee.org/document/7780940},
	doi={10.1109/CVPR.2016.571}
}
@article{RefWorks:RefID:37-ghorbani2019automatic,
	author={Amirata Ghorbani and James Wexler and James Zou and Been Kim},
	year={2019},
	month={Feb 6,},
	title={Towards Automatic Concept-based Explanations},
	abstract={Interpretability has become an important topic of research as more machine
learning (ML) models are deployed and widely used to make important decisions.
Most of the current explanation methods provide explanations through feature
importance scores, which identify features that are important for each
individual input. However, how to systematically summarize and interpret such
per sample feature importance scores itself is challenging. In this work, we
propose principles and desiderata for \emph{concept} based explanation, which
goes beyond per-sample features to identify higher-level human-understandable
concepts that apply across the entire dataset. We develop a new algorithm, ACE,
to automatically extract visual concepts. Our systematic experiments
demonstrate that \alg discovers concepts that are human-meaningful, coherent
and important for the neural network's predictions.},
	url={https://arxiv.org/abs/1902.03129}
}
@article{RefWorks:RefID:36-yeh2019completeness-aware,
	author={Chih-Kuan Yeh and Been Kim and Sercan O. Arik and Chun-Liang Li and Tomas Pfister and Pradeep Ravikumar},
	year={2019},
	month={Oct 17,},
	title={On Completeness-aware Concept-Based Explanations in Deep Neural Networks},
	abstract={Human explanations of high-level decisions are often expressed in terms of
key concepts the decisions are based on. In this paper, we study such
concept-based explainability for Deep Neural Networks (DNNs). First, we define
the notion of completeness, which quantifies how sufficient a particular set of
concepts is in explaining a model's prediction behavior based on the assumption
that complete concept scores are sufficient statistics of the model prediction.
Next, we propose a concept discovery method that aims to infer a complete set
of concepts that are additionally encouraged to be interpretable, which
addresses the limitations of existing methods on concept explanations. To
define an importance score for each discovered concept, we adapt game-theoretic
notions to aggregate over sets and propose ConceptSHAP. Via proposed metrics
and user studies, on a synthetic dataset with apriori-known concept
explanations, as well as on real-world image and language datasets, we validate
the effectiveness of our method in finding concepts that are both complete in
explaining the decisions and interpretable. (The code is released at
https://github.com/chihkuanyeh/concept_exp)},
	url={https://arxiv.org/abs/1910.07969}
}
@article{RefWorks:RefID:35-koh2020concept,
	author={Pang Wei Koh and Thao Nguyen and Yew Siang Tang and Stephen Mussmann and Emma Pierson and Been Kim and Percy Liang},
	year={2020},
	month={Jul 9,},
	title={Concept Bottleneck Models},
	abstract={We seek to learn models that we can interact with using high-level concepts:
if the model did not think there was a bone spur in the x-ray, would it still
predict severe arthritis? State-of-the-art models today do not typically
support the manipulation of concepts like "the existence of bone spurs", as
they are trained end-to-end to go directly from raw input (e.g., pixels) to
output (e.g., arthritis severity). We revisit the classic idea of first
predicting concepts that are provided at training time, and then using these
concepts to predict the label. By construction, we can intervene on these
concept bottleneck models by editing their predicted concept values and
propagating these changes to the final prediction. On x-ray grading and bird
identification, concept bottleneck models achieve competitive accuracy with
standard end-to-end models, while enabling interpretation in terms of
high-level clinical concepts ("bone spurs") or bird attributes ("wing color").
These models also allow for richer human-model interaction: accuracy improves
significantly if we can correct model mistakes on concepts at test time.},
	url={https://arxiv.org/abs/2007.04612}
}
@inproceedings{RefWorks:RefID:34-dietz2012taxolearn,
	author={Emmanuelle-Anna Dietz and Damir Vandic and Flavius Frasincar},
	year={Dec 4, 2012},
	title={TaxoLearn},
	series={WI-IAT '12},
	publisher={IEEE Computer Society},
	volume={1},
	pages={58-65},
	abstract={Building domain taxonomies is a crucial task in the domain of ontology construction. Domain taxonomy learning keeps getting more important as a form of automatically obtaining a knowledge representation of a certain domain. The alternative of manually developing domain taxonomies is not trivial. The main issues encountered when manually developing a taxonomy are the non-availability of a domain knowledge expert and the considerable amount of effort needed for this task. This paper proposes Taxo Learn, an approach to automatic construction of domain taxonomies. Taxo Learn is a new methodology that combines aspects from existing approaches, but also contains new steps in order to improve the quality of the resulted domain taxonomy. The contribution of this paper is threefold. First, we employ a word sense disambiguation step when detecting concepts in the text. Second, we show the use of semantics-based hierarchical clustering for the purpose of taxonomy learning. Third, we propose a novel dynamic labeling procedure for the concept clusters. We evaluate our approach by comparing the machine generated taxonomy with a manually constructed golden taxonomy. Based on a corpus of documents in the field of financial economics, Taxo Learn shows a high precision for the learned taxonomic concept relationships.},
	url={http://dl.acm.org/citation.cfm?id=2457664},
	doi={10.1109/WI-IAT.2012.129}
}
@article{RefWorks:RefID:33-anoop2019extracting,
	author={V. S. Anoop and S. Asharaf},
	year={2019},
	month={Sep 25,},
	title={Extracting Conceptual Relationships and Inducing Concept Lattices from Unstructured Text},
	journal={Journal of intelligent systems},
	volume={28},
	number={4},
	pages={669-681},
	abstract={Concept and relationship extraction from unstructured text data plays a key role in meaning aware computing paradigms, which make computers intelligent by helping them learn, interpret, and synthesis information. These concepts and relationships leverage knowledge in the form of ontological structures, which is the backbone of semantic web. This paper proposes a framework that extracts concepts and relationships from unstructured text data and then learns lattices that connect concepts and relationships. The proposed framework uses an off-the-shelf tool for identifying common concepts from a plain text corpus and then implements machine learning algorithms for classifying common relations that connect those concepts. Formal concept analysis is then used for generating concept lattices, which is a proven and principled method of creating formal ontologies that aid machines to learn things. A rigorous and structured experimental evaluation of the proposed method on real-world datasets has been conducted. The results show that the newly proposed framework outperforms state-of-the-art approaches in concept extraction and lattice generation.},
	isbn={0334-1860},
	url={http://www.degruyter.com/doi/10.1515/jisys-2017-0225},
	doi={10.1515/jisys-2017-0225}
}
@article{RefWorks:RefID:32-cimiano2005learning,
	author={P. Cimiano and A. Hotho and S. Staab},
	year={2005},
	month={Aug 1,},
	title={Learning Concept Hierarchies from Text Corpora using Formal Concept Analysis},
	journal={The Journal of artificial intelligence research},
	volume={24},
	pages={305-339},
	abstract={We present a novel approach to the automatic acquisition of taxonomies or concept hierarchies from a text corpus. The approach is based on Formal Concept Analysis (FCA), a method mainly used for the analysis of data, i.e. for investigating and processing explicitly given information. We follow Harris' distributional hypothesis and model the context of a certain term as a vector representing syntactic dependencies which are automatically acquired from the text corpus with a linguistic parser. On the basis of this context information, FCA produces a lattice that we convert into a special kind of partial order constituting a concept hierarchy. The approach is evaluated by comparing the resulting concept hierarchies with hand-crafted taxonomies for two domains: tourism and finance. We also directly compare our approach with hierarchical agglomerative clustering as well as with Bi-Section-KMeans as an instance of a divisive clustering algorithm. Furthermore, we investigate the impact of using different measures weighting the contribution of each attribute as well as of applying a particular smoothing technique to cope with data sparseness.},
	isbn={1076-9757},
	url={https://search.proquest.com/docview/2554129817},
	doi={10.1613/jair.1648}
}
@book{RefWorks:RefID:31-ganter2012formal,
	author={Bernhard Ganter and Rudolf Wille},
	year={2012},
	title={Formal Concept Analysis},
	publisher={Springer Berlin / Heidelberg},
	address={Berlin},
	abstract={Formal Concept AllalY.5is is a field of applied mathematics based on the math­ ematization of concept and conceptual hierarchy. It thereby activates math­ ematical thinking for conceptual data analysis and knowledge processing. The underlying notion of "concept" evolved early in the philosophical theory of concepts and still has effects today. For example, it has left its mark in the German standards DIN 2:)30 and DIN 2;3:)1. In mathematics it played a special role during the emergence of mathematical logic in the 19th century. Subsequently, however, it had virtually no impact on mathematical thinking. It was not until 1979 that the topic was revisited and treated more thoroughly. Since then, through a large number of contributions, Formal Concept Analysis has obtained such breadth that a systematic presentation is urgently needed, but can no longer be realized in one volume. Therefore, the present book foruse:':! on the mathematical foundations of Formal Concept Analysis, which ran be regarded chiefly as a branch of ap­ plied lattice theory. A series of examples serves to demonstrate the utility of the lnathematical definitions and results; in particular, to show how Formal Concept Analysis can be used for the conceptual unfolding of data contexts. These examples do not play the role of case studies in data analysis. A is intended for a comprehensive treatment of methods of separate volume conceptual data and knowledge processing. The general foundations of For­ mal Concept Analysis will also be treated separately.},
	isbn={9783540627715},
	url={https://ebookcentral.proquest.com/lib/[SITE_ID]/detail.action?docID=1263038}
}
@book{RefWorks:RefID:30-mackay2004information,
	author={David J. C. MacKay},
	year={2004},
	title={Information theory, inference, and learning algorithms},
	publisher={Cambridge Univ. Press},
	address={Cambridge [ua.]},
	edition={Reprinted with corr.},
	isbn={9780521642989},
	url={http://www.loc.gov/catdir/toc/cam031/2003055133.html}
}
@article{RefWorks:RefID:29-mrini2019rethinking,
	author={Khalil Mrini and Franck Dernoncourt and Quan Tran and Trung Bui and Walter Chang and Ndapa Nakashole},
	year={2019},
	month={Nov 10,},
	title={Rethinking Self-Attention: Towards Interpretability in Neural Parsing},
	abstract={Attention mechanisms have improved the performance of NLP tasks while
allowing models to remain explainable. Self-attention is currently widely used,
however interpretability is difficult due to the numerous attention
distributions. Recent work has shown that model representations can benefit
from label-specific information, while facilitating interpretation of
predictions. We introduce the Label Attention Layer: a new form of
self-attention where attention heads represent labels. We test our novel layer
by running constituency and dependency parsing experiments and show our new
model obtains new state-of-the-art results for both tasks on both the Penn
Treebank (PTB) and Chinese Treebank. Additionally, our model requires fewer
self-attention layers compared to existing work. Finally, we find that the
Label Attention heads learn relations between syntactic categories and show
pathways to analyze errors.},
	url={https://arxiv.org/abs/1911.03875}
}
@book{RefWorks:RefID:28-jurafsky2014speech,
	author={Dan Jurafsky and James H. Martin},
	year={2014},
	title={Speech and language processing},
	publisher={Prentice Hall, Pearson Education International},
	address={Upper Saddle River, NJ [u.a.]},
	edition={2. ed., Pearson new internat. ed.},
	isbn={9781292025438},
	url={http://bvbr.bib-bvb.de:8991/F?func=service&doc_library=BVB01&local_base=BVB01&doc_number=026203617&sequence=000002&line_number=0001&func_code=DB_RECORDS&service_type=MEDIA}
}
@misc{RefWorks:RefID:27-subject,
	title={Subject vs Predicate},
	url={https://www.thesaurus.com/e/grammar/subject-vs-predicate/}
}
@misc{RefWorks:RefID:26-spacy,
	title={Spacy Benchmarks},
	url={https://spacy.io/usage/facts-figures#benchmarks}
}
@misc{RefWorks:RefID:25-spacy,
	title={Spacy POS Tagging},
	url={https://spacy.io/usage/linguistic-features#pos-tagging}
}
@misc{RefWorks:RefID:24-spacy,
	title={Spacy},
	url={https://spacy.io/}
}
@book{RefWorks:RefID:23-lifschitz2019answer,
	author={Vladimir Lifschitz},
	year={2019},
	title={Answer set programming},
	publisher={Springer},
	address={Cham},
	abstract={Answer set programming (ASP) is a programming methodology oriented towards combinatorial search problems.  In such a problem, the goal is to find a solution among a large but finite number of possibilities. The idea of ASP came from research on artificial intelligence and computational logic. ASP is a form of declarative programming: an ASP program describes what is counted as a solution to the problem, but does not specify an algorithm for solving it.  Search is performed by sophisticated software systems called answer set solvers.Combinatorial search problems often arise in science and technology, and ASP has found applications in diverse areas-in historical linguistic, in bioinformatics, in robotics, in space exploration, in oil and gas industry, and many others.  The importance of this programming method was recognized by the Association for the Advancement of Artificial Intelligence in 2016, when AI Magazine published a special issue on answer set programming.
The book introduces the reader to the theory and practice of ASP. It describes the input language of the answer set solver CLINGO, which was designed at the University of Potsdam in Germany and is used today by ASP programmers in many countries.  It includes numerous examples of ASP programs and present the mathematical theory that ASP is based on.  There are many exercises with complete solutions.},
	isbn={9783030246570}
}
@misc{RefWorks:RefID:22-clingo,
	title={Clingo ASP solver},
	url={https://potassco.org/clingo/}
}
@article{RefWorks:RefID:21-fitting1992michael,
	author={Melvin Fitting},
	year={1992},
	month={Mar},
	title={Michael Gelfond and Vladimir Lifschitz. The stable model semantics for logic programming. Logic programming, Proceedings of the fifth international conference and symposium, Volume 2, edited by Robert A. Kowalski and Kenneth A. Bowen, Series in logic programming, The MIT Press, Cambridge, Mass., and London, 1988, pp. 1070–1080. - Kit Fine. The justification of negation as failure. Logic, methodology and philosophy of science VIII, Proceedings of the Eighth International Congress of Logic, Method},
	journal={The Journal of symbolic logic},
	volume={57},
	number={1},
	pages={274-277},
	isbn={0022-4812},
	doi={10.2307/2275201}
}
@article{RefWorks:RefID:19-law2020fastlas:,
	author={Mark Law and Alessandra Russo and Elisa Bertino and Krysia Broda and Jorge Lobo},
	year={2020},
	month={Apr 3,},
	title={FastLAS: Scalable Inductive Logic Programming Incorporating Domain-Specific Optimisation Criteria},
	journal={Proceedings of the ... AAAI Conference on Artificial Intelligence},
	volume={34},
	number={3},
	pages={2877-2885},
	abstract={Inductive Logic Programming (ILP) systems aim to find a set of logical rules, called a hypothesis, that explain a set of examples. In cases where many such hypotheses exist, ILP systems often bias towards shorter solutions, leading to highly general rules being learned. In some application domains like security and access control policies, this bias may not be desirable, as when data is sparse more specific rules that guarantee tighter security should be preferred. This paper presents a new general notion of a scoring function over hypotheses that allows a user to express domain-specific optimisation criteria. This is incorporated into a new ILP system, called FastLAS, that takes as input a learning task and a customised scoring function, and computes an optimal solution with respect to the given scoring function. We evaluate the accuracy of FastLAS over real-world datasets for access control policies and show that varying the scoring function allows a user to target domain-specific performance metrics. We also compare FastLAS to state-of-the-art ILP systems, using the standard ILP bias for shorter solutions, and demonstrate that FastLAS is significantly faster and more scalable.},
	isbn={2159-5399},
	doi={10.1609/aaai.v34i03.5678}
}
@article{RefWorks:RefID:18-law2020ilasp,
	author={Mark Law and Alessandra Russo and Krysia Broda},
	year={2020},
	month={May 2,},
	title={The ILASP system for Inductive Learning of Answer Set Programs},
	abstract={The goal of Inductive Logic Programming (ILP) is to learn a program that
explains a set of examples in the context of some pre-existing background
knowledge. Until recently, most research on ILP targeted learning Prolog
programs. Our own ILASP system instead learns Answer Set Programs, including
normal rules, choice rules and hard and weak constraints. Learning such
expressive programs widens the applicability of ILP considerably; for example,
enabling preference learning, learning common-sense knowledge, including
defaults and exceptions, and learning non-deterministic theories. In this
paper, we first give a general overview of ILASP's learning framework and its
capabilities. This is followed by a comprehensive summary of the evolution of
the ILASP system, presenting the strengths and weaknesses of each version, with
a particular emphasis on scalability.},
	url={https://arxiv.org/abs/2005.00904}
}
@misc{RefWorks:RefID:16-2021automatic,
	year = 	 {2021},
	month = 	 {Oct},
	title = 	 {Automatic Concept Extraction for Concept Bottleneck-based Video Classification},
	abstract = 	 {Recent efforts in interpretable deep learning models have shown that concept-based explanation methods achieve competitive accuracy with standard end-to-end models and enable reasoning and intervention about extracted high-level visual concepts from images, e.g., identifying the wing color and beak length for bird-species classification. However, these concept bottleneck models rely on a domain expert providing a necessary and sufficient set of concepts-which is intractable for complex tasks such as video classification. For complex tasks, the labels and the relationship between visual elements span many frames, e.g., identifying a bird flying or catching prey-necessitating concepts with various levels of abstraction. To this end, we present CODEX, an automatic Concept Discovery and Extraction module that rigorously composes a necessary and sufficient set of concept abstractions for concept-based video classification. CoDEx identifies a rich set of complex concept abstractions from natural language explanations of videos-obviating the need to predefine the amorphous set of concepts. To demonstrate our method&#39;s viability, we construct two new public datasets that combine existing complex video classification datasets with short, crowd-sourced natural language explanations for their labels. Our method elicits inherent complex concept abstractions in natural language to generalize concept-bottleneck methods to complex tasks.},
	url = 	 {https://openreview.net/pdf?id=66kgCIYQW3}
}
@misc{RefWorks:RefID:14-carreira2019short,
	author = 	 {Joao Carreira and Eric Noland and Chloe Hillier and Andrew Zisserman},
	year = 	 {2019},
	month = 	 {Jul 15,},
	title = 	 {A Short Note on the Kinetics-700 Human Action Dataset},
	abstract = 	 {We describe an extension of the DeepMind Kinetics human action dataset from 600 classes to 700 classes, where for each class there are at least 600 video clips from different YouTube videos. This paper details the changes introduced for this new release of the dataset, and includes a comprehensive set of statistics as well as baseline results using the I3D neural network architecture.},
	url = 	 {https://explore.openaire.eu/search/publication?articleId=od________18::2a62c0566d4ac6facf800e04a34462f8}
}
@misc{RefWorks:RefID:13-mullner2011modern,
	author = 	 {Daniel Müllner},
	year = 	 {2011},
	month = 	 {Sep 12,},
	title = 	 {Modern hierarchical, agglomerative clustering algorithms},
	abstract = 	 {This paper presents algorithms for hierarchical, agglomerative clustering which perform most efficiently in the general-purpose setup that is given in modern standard software. Requirements are: (1) the input data is given by pairwise dissimilarities between data points, but extensions to vector data are also discussed (2) the output is a "stepwise dendrogram", a data structure which is shared by all implementations in current standard software. We present algorithms (old and new) which perform clustering in this setting efficiently, both in an asymptotic worst-case analysis and from a practical point of view. The main contributions of this paper are: (1) We present a new algorithm which is suitable for any distance update scheme and performs significantly better than the existing algorithms. (2) We prove the correctness of two algorithms by Rohlf and Murtagh, which is necessary in each case for different reasons. (3) We give well-founded recommendations for the best current algorithms for the various agglomerative clustering schemes.},
	url = 	 {https://explore.openaire.eu/search/publication?articleId=od________18::b323e546e5152b4b48ac7e1352f1cca7}
}
@article{RefWorks:RefID:10-abu-el-haija2016youtube-8m:,
	author={Sami Abu-El-Haija and Nisarg Kothari and Joonseok Lee and Paul Natsev and George Toderici and Balakrishnan Varadarajan and Sudheendra Vijayanarasimhan},
	year={2016},
	month={Sep 27,},
	title={YouTube-8M: A Large-Scale Video Classification Benchmark},
	abstract={Many recent advancements in Computer Vision are attributed to large datasets.
Open-source software packages for Machine Learning and inexpensive commodity
hardware have reduced the barrier of entry for exploring novel approaches at
scale. It is possible to train models over millions of examples within a few
days. Although large-scale datasets exist for image understanding, such as
ImageNet, there are no comparable size video classification datasets.
In this paper, we introduce YouTube-8M, the largest multi-label video
classification dataset, composed of ~8 million videos (500K hours of video),
annotated with a vocabulary of 4800 visual entities. To get the videos and
their labels, we used a YouTube video annotation system, which labels videos
with their main topics. While the labels are machine-generated, they have
high-precision and are derived from a variety of human-based signals including
metadata and query click signals. We filtered the video labels (Knowledge Graph
entities) using both automated and manual curation strategies, including asking
human raters if the labels are visually recognizable. Then, we decoded each
video at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to
extract the hidden representation immediately prior to the classification
layer. Finally, we compressed the frame features and make both the features and
video-level labels available for download.
We trained various (modest) classification models on the dataset, evaluated
them using popular evaluation metrics, and report them as baselines. Despite
the size of the dataset, some of our models train to convergence in less than a
day on a single machine using TensorFlow. We plan to release code for training
a TensorFlow model and for computing metrics.},
	url={https://arxiv.org/abs/1609.08675}
}
@article{RefWorks:RefID:8-yan2022multiview,
	author={Shen Yan and Xuehan Xiong and Anurag Arnab and Zhichao Lu and Mi Zhang and Chen Sun and Cordelia Schmid},
	year={2022},
	month={Jan 11,},
	title={Multiview Transformers for Video Recognition},
	abstract={Video understanding requires reasoning at multiple spatiotemporal resolutions
-- from short fine-grained motions to events taking place over longer
durations. Although transformer architectures have recently advanced the
state-of-the-art, they have not explicitly modelled different spatiotemporal
resolutions. To this end, we present Multiview Transformers for Video
Recognition (MTV). Our model consists of separate encoders to represent
different views of the input video with lateral connections to fuse information
across views. We present thorough ablation studies of our model and show that
MTV consistently performs better than single-view counterparts in terms of
accuracy and computational cost across a range of model sizes. Furthermore, we
achieve state-of-the-art results on five standard datasets, and improve even
further with large-scale pretraining. We will release code and pretrained
checkpoints.},
	url={https://arxiv.org/abs/2201.04288}
}
@inproceedings{RefWorks:RefID:9-carreira2017quo,
	author={Joao Carreira and Andrew Zisserman},
	year={Jul 2017},
	title={Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset},
	publisher={IEEE},
	pages={4724-4733},
	abstract={The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.2\% on HMDB-51 and 97.9\% on UCF-101.},
	isbn={1063-6919},
	url={https://ieeexplore.ieee.org/document/8099985},
	doi={10.1109/CVPR.2017.502}
}
@inbook{RefWorks:RefID:4-mao2019hierarchical,
	author={Feng Mao and Xiang Wu and Hui Xue and Rong Zhang},
	year={2019},
	month={Jan 23,},
	title={Hierarchical Video Frame Sequence Representation with Deep Convolutional Graph Network},
	series={Computer Vision – ECCV 2018 Workshops},
	publisher={Springer International Publishing},
	address={Cham},
	pages={262-270},
	abstract={High accuracy video label prediction (classification) models are attributed to large scale data. These data could be frame feature sequences extracted by a pre-trained convolutional-neural-network, which promote the efficiency for creating models. Unsupervised solutions such as feature average pooling, as a simple label-independent parameter-free based method, has limited ability to represent the video. While the supervised methods, like RNN, can greatly improve the recognition accuracy. However, the video length is usually long, and there are hierarchical relationships between frames across events in the video, the performance of RNN based models are decreased. In this paper, we proposes a novel video classification method based on a deep convolutional graph neural network (DCGN). The proposed method utilize the characteristics of the hierarchical structure of the video, and performed multi-level feature extraction on the video frame sequence through the graph network, obtained a video representation reflecting the event semantics hierarchically. We test our model on YouTube-8M Large-Scale Video Understanding dataset, and the result outperforms RNN based benchmarks.},
	isbn={9783030110178},
	url={http://link.springer.com/10.1007/978-3-030-11018-5_24},
	doi={10.1007/978-3-030-11018-5_24}
}
@inproceedings{RefWorks:RefID:3-piergiovanni2018fine-grained,
	author={AJ Piergiovanni and Michael S. Ryoo},
	year={Jun 2018},
	title={Fine-Grained Activity Recognition in Baseball Videos},
	publisher={IEEE},
	pages={1821-18218},
	abstract={In this paper, we introduce a challenging new dataset, MLB-YouTube, designed for fine-grained activity detection. The dataset contains two settings: segmented video classification as well as activity detection in continuous videos. We experimentally compare various recognition approaches capturing temporal structure in activity videos, by classifying segmented videos and extending those approaches to continuous videos. We also compare models on the extremely difficult task of predicting pitch speed and pitch type from broadcast baseball videos. We find that learning temporal structure is valuable for fine-grained activity recognition.},
	url={https://ieeexplore.ieee.org/document/8575389},
	doi={10.1109/CVPRW.2018.00226}
}
@misc{RefWorks:RefID:2-qi2018universal,
	author = 	 {Peng Qi and Timothy Dozat and Yuhao Zhang and Christopher D. Manning},
	year = 	 {2018},
	title = 	 {Universal Dependency Parsing from Scratch},
	journal = 	 {Proceedings of the},
	abstract = 	 {This paper describes Stanford&#39;s system at the CoNLL 2018 UD Shared Task. We introduce a complete neural pipeline system that takes raw text as input, and performs all tasks required by the shared task, ranging from tokenization and sentence segmentation, to POS tagging and dependency parsing. Our single system submission achieved very competitive performance on big treebanks. Moreover, after fixing an unfortunate bug, our corrected system would have placed the 2 nd , 1 st , and 3 rd on the official evaluation metrics LAS, MLAS, and BLEX, and would have out-performed all submission systems on low-resource treebank categories on all metrics by a large margin. We further show the effectiveness of different model components through extensive ablation studies.},
	doi={10.18653/v1/k18-2016}
}
@inproceedings{RefWorks:RefID:1-lifschitz2008answer,
	author={Vladimir Lifschitz},
	year={July, 2008},
	month={July 13-17, 2008},
	title={What is Answer Set programming?},
	booktitle={AAAI 2008},
	abstract={Answer Set Programming},
	url={http://www.aaai.org/Library/AAAI/2008/aaai08-270.php}
}
