

% Talk about the idea of concept bottleneck
% mulit-modal data fusion for classification


% Need to mention rule learning from concepts to labels

% Lanky boy suggestions
% What is the problem?
% Why is it interesting?
% What is the main idea for solving it?

\chapter{Introduction}

% What we aim to do - we aim to develop an interpretable NN pipeline
% Why is it important - applicability in high risk domains to verify that such a solution does indeed work.

Understanding why a machine learning model makes a particular prediction is always beneficial. 
It verifies that the model has learned a correct solution instead of a spurious pattern in the training data.
Interpretability is extremely important when applying models in a high-risk environment, such as autonomous cars, where a cost of an error is extremely high.

Interpretable models do exist.
Humans can easily interpret decision trees and logic-based models to understand why they produce a specific solution.
However, they fail to reach the level of performance that end-to-end neural networks achieve.

Concept bottleneck models \cite{RefWorks:RefID:35-koh2020concept} are a novel set of models achieving comparable performance to end-to-end NNs while allowing the understanding of their prediction.
They use neural networks to predict high-level human-engineered concepts before predicting the final label.
As such, they allow reasoning about the final labels with intermediate concepts they have predicted.

In this project, we take the concept bottleneck idea further by mining a set of concepts directly from human-generated text explanations.
In addition, we further improve the model interpretability by designing a logic-based classification pipeline from intermediate concepts to final labels.


% If done successfully, given natural language explanations could even be used to explain unseen classification examples or improve the video classification performance.


\section{Motivation}

Natural language can be used as metadata for any sort of data.
To make it machine-readable and valuable for a task at hand, one needs to extract relevant pieces of information about it consistently.

However, using textual information is a challenging task.
Humans speak and write at varying levels of granularity, and a sentence may contain multiple relevant pieces of information.
For instance, consider the following explanations of the same event:
\begin{lstlisting}
The batter hit a fly ball but it was caught in the air by an outfielder.
The player struck a ball in the air. However, the ball was caught by a fielder before it touched the ground.
\end{lstlisting}
They convey similar semantic information in a very different syntactic manner.

The semantic parsing tasks showcase how challenging it is to deal with such a problem.
These tasks aim to convert a sentence into a convenient form for a machine, such as the logical form, which they can then use for other downstream tasks.
One instance of such a task is Abstract Meaning Representation Parsing \cite{RefWorks:RefID:92-banarescu2013abstract}, which converts sentences into a tree form that captures general semantic relations.
The current state of the art \cite{RefWorks:RefID:93-zhou2021structure-aware} only achieves the F1 score of 0.817, allowing room for significant improvement.

The text we attempt to deal with might be even more general than what semantic approaches capture.
We want to be able to use any declarative sentence stating the facts that have occurred.
In addition, we want to mine concepts from text that preserve part of the semantics of the original sentence, that are syntactically structured and contain relevant information with respect to the label they are targeted to explain. 
The method that we envisage to develop has to be domain-agnostic in the sense that we can apply it to different texts for different classification tasks.

Moreover, the set of concepts should be clearly interpretable.
For example, if the method were to extract a concept \emph{ball}, it may be unclear what it refers to in the baseball domain.
Does it refer to the event \emph{ball}, which occurs when a pitcher throws the ball outside of the strike zone, or simply the \emph{ball} used in a baseball game?

Taking into account the properties mentioned above, one of the goals of a project is to design a mostly syntactic concept mining framework capable of extracting concept sentences from text.
The mostly syntactic nature allows the method to be applicable in various domains.
The only semantic part involves grouping semantically equivalent sentences using transformers, which is also domain-independent.

% The idea for solving the challenging the problems mentioned above consists of purely syntactic \textbf{atomisation} and \textbf{generalisation} stages.
% They present the text information in a much more consistent form.
% The former extracts atomic sentences from the data, sentences which convey only one piece of information.
% The latter finds all concept sentences from atomic sentences, which are grammatically correct sentences and have a varying degree of information of the initial atomic sentence.
% For example, atomisation would convert a sentence in the following manner:
% \begin{lstlisting}
% The batter hit a fly ball but it was caught in the air by an outfielder.
%  $\rightarrow$
% The batter hit a fly ball. It was caught in the air by an outfielder.
% \end{lstlisting}
% It captures two events which occurred as separate atomic sentences.
% Moreover, Generalisation would convert an atomic sentence in this manner:
% \begin{lstlisting}
% It was caught in the air by an outfielder.
%  $\rightarrow$
% It was caught.
% It was caught in the air.
% It was caught by an outfielder.
% It was caught in the air by an outfielder.
% \end{lstlisting}
% Any of these concept sentences may convey the ideal level of granularity for a task.

% Notice that the concept sentences capture all of the three criteria outlined; they are domain independent as they are syntactic, capture information at any level of granularity, and are interpretable since they are full valid sentences.

% Once concept sentences can be mined from text given as meta-data of labels in a given task, such concepts can be used to improve the interpretability of the concept bottleneck method.
% The original concept bottleneck work \cite{RefWorks:RefID:35-koh2020concept} provides a way to highlight what concepts a trained classification model given higher weight, but cannot explain what concepts are used to compute the final predictions.
% The second main goal of this project is to develop a fully interpretable classification framework which can solve a classification task using mined concepts and their probability of occurring.
% It should also be able to explicitly identify dependencies between concepts and a downstream label.

% As such, this project aims to propose a novel method, combining techniques from natural language processing, deep learning and logic-based learning, that would extract domain-independent concept sentences.

\section{Objectives}

This project proposes a novel method that combines techniques from natural language processing, deep learning, and logic-based learning to develop an interpretable and general high-quality classifier.
It builds upon the work by Jeyakumar et al. \cite{RefWorks:RefID:16-2021automatic}, which develops a concept mining framework trained with a concept bottleneck model for the baseball classification problem. 
It also provides novel solutions that are applicable to other domains.
The project addresses the following high-level objectives:

% objectives
%  - concept mining 
%  - improved explainability
%  - explore the generality of the methods

\begin{itemize}
    \item Develop a general framework for mining concept sentences --- The goal is to extract concept sentences from declarative sentences successfully. Such a framework should capture information at various levels of granularity and be domain-independent.
    
    \item Improve the explainability of the concept bottleneck model ---  The existing approach provided the explanations by choosing the top three concepts with the highest attention score of a trained model.
    We aim to improve it by enhancing the explanation quality and simplifying the interpretation of the explanations.
    
    \item Explore the generality of the methods. --- We explore whether our proposed solutions can be translated to different domains.
    
\end{itemize}

\section{Challenges}

Several key challenges need to be handled as part of this project in order to achieve the above objectives:
\begin{enumerate}
    \item \emph{How should a concept mined from natural language be defined?} Mining a concept that is helpful and immediately extensible to other domains may be challenging. It is hard because expert knowledge cannot be used to help craft features that the subsequent architecture should use.
    
    \item \emph{How should a sentence be decomposed into a concept sentence?} To allow a concept to be clearly interpretable and domain independent, we represent it in the form of a sentence.
    However, finding a solution that consistently converts a sentence into simpler ones that convey the same piece of information is challenging. 
    The main challenge is doing it well for a diverse set of sentences.
    
    \item \emph{How can a concept mining pipeline be scaled up with a large amount of data?} 
    Using logic-based learning for mining concepts from a sentence would allow the three properties of interpretability, different level of granularity of the information, and domain independence. 
    This is because it is logic-based and therefore naturally interpretable and the search space for solutions is declaratively definable.
    However, logic-based learning systems are challenging to scale.
    
    \item \emph{How can a logic-based learning approach be used to learn dependencies between mined concept sentences and downstream labels to improve explainability?} Most logic-based learning systems do not have mechanisms for dealing with probabilistic atoms.
    Each atom can either be included or not be included in a solution.
    On the other hand, each concept may be mined or predicted with some probability $p$ of it being true.
\end{enumerate}

 
\section{Contributions}

The project addresses all the challenges identified above and provides the following main contributions:
\begin{itemize}
    \item A method for mining concept sentences from declarative sentences that is domain-independent. (Chapter \ref{solving-nlp-tasks-logically})
    
    \item Integration of a concept mining approach into a concept bottleneck model utilising human-written label explanations as text metadata of the labels. (Chapter \ref{concept-bottleneck-pipeline})
    
    \item A fully interpretable, logical-based learning framework for a classification task using probabilistic facts. (Chapter \ref{logic-based-classification})
\end{itemize}

