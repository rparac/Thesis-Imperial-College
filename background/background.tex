\chapter{Background}

% Talk about ILP^opl whatever we are using (the latest one)

\section{Answer Set Programming}

Answer Set Programming  \cite{RefWorks:RefID:1-lifschitz2008answer} is a form of declarative programming, with a Prolog-like syntax suitable for solving NP-hard search problems.
However, it is based on a different computation mechanism than Prolog: stable model semantics \cite{RefWorks:RefID:21-fitting1992michael}.
Answer set solver is a program that generates stable models of an answer set program, which are solutions of an answer set program. 
The chosen answer set solver used throughout this project is clingo \cite{RefWorks:RefID:22-clingo}.

This section will briefly highlight the syntax of the answer set programs and the stable model semantics.

\subsection{Syntax}

Here are the types of rules in ASP\footnote{Disjunctive rules also exist, but they are omitted as they are not used in this work.}: 

 1. \emph{normal rule} -> $ h\; :- b_1, b_2, ..., b_n, not\; n_1, not\; n_2, ..., not\; n_o$
 
 2. \emph{hard constraint} -> $:- \; b_1, ..., b_n, not\; n_1, not\; n_2, ..., not\; n_o$
 
 3. \emph{choice rule} -> $lb\{h_1; ..., h_m\}ub\; :- \;  b_1, b_2, ..., b_n, not\; n_1, not\; n_2, ..., not\; n_o$
 
where $lb, ub$ are integers, $n_1,...,n_o$ are atoms and $b_1, ...,b_n$ are either atoms or aggregates.

Aggregates have the following syntax $lb\#agg\{e_1; ...; e_n\}ub$ where $lb, ub$ are ints, $agg$ is an aggregate function (e.g. $sum$) and $e_i$ is an aggregate element.
Additionally, aggregate element has the form  $t_1, ..., t_k : c_1, ..., c_j$ where $t_i$ is a term and $c_i$ is a literal.
 
This syntax is quite expressive. For instance, here are two ways to define a coin falling either on heads or tails.

\subsubsection{Method 1:}
\begin{verbatim}
coin(c1).
heads(C) :- coin(C), not tails(C).
tails(C) :- coin(C), not heads(C).
\end{verbatim}
\subsubsection{Method 2:}
\begin{verbatim}
coin(c1).
1 { heads(C); tails(C) } 1 :- coin(C).
\end{verbatim}

The results of these programs, i.e. the answer sets of the programs, are \{coin(c1), heads(c1)\} and \{coin(c1), tails(c1)\}.
It will be explained how they are computed in the subsequent subsection.

As seen in the example, a program can have multiple answer sets.
So, an additional piece of syntax is defined which evaluates whether some answer set is better than another.
This piece of syntax is referred to as a weak constraint.
It is of the form $:\sim {} b_1,..., b_n.[wt@lev, t1, ..., t_m]$
where $lev$ is an integer, $b_i$ a literal, and $t_i$ term.

\subsection{Semantics}

The Answer Set Programming is based on stable model semantics \cite{RefWorks:RefID:21-fitting1992michael}.

The answer set is defined as follows: 
\emph{Given a program P and a Herbrand interpretation X, X is an \textbf{answer set} of P iff X is a minimal Herbrand model of $RG(P)^X$ (reduct with respect to X).}

To understand the presented definition, one needs to know a few other concepts presented in this subsection.
Here the definitions for those concepts will mainly be explained intuitively rather than formally; for deeper understanding and more formal definitions, please refer to the \emph{Answer Set Programming} book \cite{RefWorks:RefID:23-lifschitz2019answer}. \\

\textbf{Herbrand interpretation} a program is an assignment of every element of the Herbrand base of that program to either true or false. \textbf{Herbrand base} of a program P is a set of all ground atoms that can be made using constants, predicate symbols, and functions of a program P.\\

When a Herbrand interpretation X satisfies all the rules in of a program, X is the \textbf{Herbrand model} of a program. X is also a minimal Herbrand model if there is no smaller subset X', also a Herbrand model.\\

\textbf{Relevant grounding} replaces variables with a program with constants available.
It iteratively fills up the rules by adding every rule whose elements of $body^+$(R) are heads of already included rules. 
The $body^+$ constraint ignores the $not$ atoms. 
For example, take P = \{p(X) :- q(X). q(a).\}. 
The first iteration of relevant grounding would create P' = \{q(a).\} while after the second one P' = \{q(a). p(a) :- q(a).\}\\

The \textbf{reduct of a program} P with respect to X ($P^X$) is a construct used to remove the $not\: c$ terms from a program.
Computing the reduct is done in the following manner. The solver assumes the X is a solution and iterates through every $not\: c$ within P. 
If $c$ is not in X (assumed to be false), then the $not\: c$ term is removed from the rule containing it. 
It essentially removes the need to worry about $not\: c$ since that term is satisfied.
On the other hand, if $c$ is in X, then the entire rule containing $not\: c$ is removed.
There is no need to consider the rule whose body is false since it cannot be satisfied.

Constructing the reduct of a \textbf{choice rule} has an additional step.
It is turned into either normal rules or a constraint.
Given a possible interpretation $X$, the choice rule $lb\{h_1; ..., h_m\}ub\; :- \;  b_1, b_2, ..., b_n, not\; n_1, ..., not\; n_o$ is converted in a following manner:

1. if $ lb \leq |\{h_1; ...; h_m\} \cap X| \leq ub$ then $h_i :- \;  b_1, b_2, ..., b_n, not\; n_1, ..., not\; n_o$ for is created for each $i \in \{1..m\}$

2. Otherwise, a constraint of the form $ :- \;  b_1, b_2, ..., b_n, not\; n_1, not\; n_2, ..., not\; n_o$ 
is created.\\

So, to check whether X is an answer set, one needs to compute a relevant grounding of a program. Then it needs to construct a reduct of that program with respect to X.
From the reduct, one can easily construct a minimal model M by starting from facts and iteratively adding heads of those rules with all body elements already in M.
Finally, if X = M, then X is an answer set.
Note that the head of a \textbf{constraint} is considered to be $\bot$. 
Hence, if the body of a constraint is satisfied, $\bot$ is added to M, so it cannot be equal to X.
In this manner, the constraints eliminate possible answer sets.






% Introduction to stable model semantics and its benefits over other forms of programming.
% Touch upon non-monotonicity. Its non-Turing completeness.

\section{Inductive Logic Programming}

% What is Inductive Logic Programming
Inductive Logic Programming \cite{RefWorks:RefID:42-muggleton1991inductive} is a field at an intersection of machine learning and logic programming.
In most cases, the goal of an inductive learning task is to learn a set of rules (a hypothesis H), which combined with the background knowledge B can entail every positive example while not entailing any negative example. 
Some systems use a weakened version of that statement to allow for noisy examples, such as the newer versions of ILASP \cite{RefWorks:RefID:55-law2018inductive}.

Many ILP systems have been developed over the years, such as Progol5 \cite{RefWorks:RefID:43-muggleton2000theory}, HAIL \cite{RefWorks:RefID:44-ray2003hybrid}, TopLog \cite{RefWorks:RefID:45-muggletontoplog:}, and TAL \cite{RefWorks:RefID:46-corapi2010inductive}.

The chosen system for this project is ILASP \cite{RefWorks:RefID:18-law2020ilasp}, a system that does the inductive learning of the answer sets programs.
Choosing a system that learns ASP is done because the ASP environment can effectively perform non-monotonic reasoning.\\

To understand the ILASP system fully, a few more definitions need to be introduced.

An atom A is \textbf{bravely entailed} if by a logic program P if it is included (true) in at least one answer set of P. 
On the other hand, an atom A is \textbf{cautiously entailed} if it is included (true) in all answer sets of P.

A pair $E = <E^{inc}, E^{exc}>$ (<inclusions, exclusions>) is a partial interpretation of sets of literals.
Answer set X extends E iff set of inclusions is a subset of X ($E^{inc} \subseteq X$) and X is disjoint with exclusions ($E^{exc} \cap A = \emptyset$).


The \textbf{language(inductive) bias} M, is a a pair $<M_h, M_b>$ which is used to construct the \textbf{search space} $S_M$ of a learning task.
The rule of the form  \\$ h \;{:-} b_1, b_2, ..., b_n, not\; n_1, not\; n_2, ..., not\; n_o$ is in $S_M$ iff:
 
 a) $h$ is compatible with $M_h$. This either means that $h$ is an atom compatible with $M_h$, an aggregate $lb\{h_1,.., h_m\}ub$ where each atom $h_i$ is compatible with $M_h$ or $h$ is empty.
 
 b) $b_i$ and $n_i$ are compatible with $M_b$.
 
 c) no variable does not appear in at least one element  of a positive body literal (the rule is safe). \\ 
 
Understanding what compatible refers to in the previous definition is the easiest through the means of an example.

\subsubsection{Inductive bias example}
\begin{verbatim}
#modeh(heads(var(coin))).
#modeh(tails(var(coin))).
#modeha(heads(var(coin))).
#modeha(tails(var(coin))).

#modeb(heads(var(coin))).
#modeb(tails(var(coin))).
#modeb(coin(var(coin))).
#modeb(coin(const(coin))).

#constant(coin, c1).
\end{verbatim}

The example above is a possible definition of the inductive bias for the simple coin problem written in ILASP-compatible syntax.
The \#modeh directive specifies that an atom can occur in the head of the rule. In this case, it defines that $heads(X)$ is compatible with $M_h$.
Similarly, the \#modeha defines that an atom can occur in the aggregate of the rule. Hence, it determines that  $lb\{..., heads(X), ...\}ub$ is a head compatible to $M_h$.
The \#modeb syntax defines that an atom can occur in the body either as a positive or negative atom.
Finally, the \#constant defines constants that can replace const functions.

Let $S_M$ be the search space constructed from the language bias M, B background knowledge, $E^+$/$E^-$ set of positive/negative examples, T = $<B, S_M, E^+, E^->$ is the \textbf{learning from answer set task} \cite{RefWorks:RefID:47-law2014inductive}.
The hypothesis H is a set of inductive solution iff:

1. It is a subset of search space $S_M$.

2. For every negative example $e^-$, there is no answer set A of the program $B \cap H$ which extends that example.

3. For every positive example $e^+$, there is an answer set A of the program $B \cap H$ which extends it.
\\

Notice that the positive examples are bravely entailed while the negative are cautiously entailed.\\

\subsubsection{Examples in ILASP}
\label{examples-in-ilasp}

\begin{verbatim}
#pos(
{heads(c1)},
{tails(c1)},
{coin(c1).}
).

#neg(
{tails(c2), heads(c2)},
{},
{coin(c2).}
).
\end{verbatim}

The shown piece of code is a simple definition of examples in ILASP. 
Both positive and negative take either 2 or 3 parameters. The first two are the set of inclusions $E^{inc}$ and the set of exclusions $E^{exc}$.
The third optional parameter is the context $C_e$ of an example.
The context of an example, $C_e$ is a set of rules and facts that are only relevant when explaining the example $e$.
With this parameter, the agent tries to find hypothesis H such that an answer set of $B \cap H \cap C_e$ extends $e$ (for positive $e$).
The analogue holds for negative examples. 
A formal definition of the Context-dependent Learning from Ordered Answer Sets framework can be found in work by Broda et al., \cite{RefWorks:RefID:56-broda2016iterative} which introduced context-dependent examples to ILASP. \\

FastLAS \cite{RefWorks:RefID:19-law2020fastlas:}, a much faster alternative to ILASP, has been developed recently.
Unfortunately, to get to the desired level of scale, the system operates with certain constraints which are too restrictive for this project.
For example, the programs constructed will generate at most one answer set.

% Link to Tutorials for FastLas, ILASP

\section{Natural Language Processing}

The critical requirement of this project is to extract syntactic concept sentences from the text corpus.
This section has a brief overview of the techniques applied to make the concept extraction possible.

The techniques presented in the following section are implemented by a popular library \emph{spacy} \cite{RefWorks:RefID:24-spacy} which has been utilised in this project. 
The language model currently used throughout the project is \emph{en\_wb\_gl\_lg}, but a more performant option may replace it after further investigation.

\subsection{Preprocessing Techniques}

\textbf{Tokenisation} separates the given text into smaller units named \emph{tokens}. 
It is common to split English text into words as tokens, as they carry meaning and are easy to extract.
The \emph{spacy} library implements such behaviour with it additionally separating punctuation into separate tokens.\\

\textbf{Sentence splitting} is a similar problem as tokenisation. It splits a text corpus into sentences before they are processed individually. \\

% TODO think of better %
\textbf{Word Normalisation} replaces words with a consistent form which greatly simplifies recognition of identical words.
Some of the reasons different forms of words arise are capitalisation (Cat, cat -> cat), acronyms (U.S.A/USA/U S A -> USA) and spelling variants (chequebook/cheque book -> cheque book). 
Most of the words can be modified by applying rule-based modifications, but there needs to be special handling of some words which have double meaning such as \emph{Turkey} (bird/country). \\


% tag spacy PoS 
\textbf{Part-of-speech tagging} attempts to determine which tag does a word have in the sentence.
The problem is much more complicated than the ones previously described as the part of speech can often be dependent on the meaning of the word in a sentence.
For example, the sentence \emph{I play the main character in a play} highlights how word \emph{play} can have two identically written words that can have different meanings and parts of speech.
The first occurrence of \emph{play} is a verb, and the second is a \emph{noun}.

The spacy library uses neural networks and statistical models to determine which tokens the library should assign which POS tag \cite{RefWorks:RefID:25-spacy}.
The classifier which spacy uses has a very high accuracy for the English language, with accuracy between 97-98\% depending on the model \cite{RefWorks:RefID:26-spacy}.


% http://nlpprogress.com/english/dependency_parsing.html - tracks state of the art

\subsection{Parsing}

\subsubsection{Dependency Parsing}

Dependency Parsing is a form of parsing that attempts to determine words' dependency within a sentence \cite{RefWorks:RefID:28-jurafsky2014speech}.

The resulting structure obtained is a dependency parse tree.
An example of a dependency parse tree can be seen in \ref{dependency-graph}.
This structure enables determining which purpose does a word serve in a sentence.
To illustrate why determining what purpose a word serves in a sentence may be helpful, here is a predicate/subject refresher:
\emph{The subject is a person/object who/what the sentence is about, while the predicate tells what the subject is or what it is doing} \cite{RefWorks:RefID:27-subject}.
From this definition, it is clear that one can easily extract an actor in a sentence if they know what it is doing.
The dependencies produced by the \emph{spacy} dependency parser are more precise as they follow \hyperlink{https://univesaldependencies.org}{Universal Dependencies}.
For instance, those dependencies distinguish between nominal and clausal subjects.
Please refer to the \hyperlink{https://univesaldependencies.org}{Universal Dependencies} to see a complete list of the dependencies available if needed. 

% How does it work? Maybe describe Mniri et al approach if lacking content

% What is the performance.
To evaluate the performance of a dependency parser, one uses labelled attachment score (LAS) and unlabelled attachment score (UAS) accuracy \cite{RefWorks:RefID:28-jurafsky2014speech}.
The former validates how well a word is assigned to its head and whether a correct dependency relation is assigned.
The latter score checks whether the head is assigned correctly.
Spacy's dependency parsing system slightly trails behind the state of the art approaches at the moment, with its UAS and LAS being at 95.1\% and 93.7\%, respectively. 
Mniri et al. \cite{RefWorks:RefID:29-mrini2019rethinking} developed the current state of the art model, with UAS/LAS scores at 97.4\% and 96.3\%.

\begin{figure}[h]
\caption{Dependency graph of \emph{The fielder caught the ball.}}
\centering
\includegraphics[width=\textwidth]{background/dependency_graph.png}
\label{dependency-graph}
\end{figure}

\subsubsection{Constituency Parsing}

Constituency Parsing is a form of syntactic parsing which assigns a structure to a sentence created by a context-free grammar \cite{RefWorks:RefID:28-jurafsky2014speech}.
This approach aims to group words into constituents, a group of words that behave as a single unit.
For example, one may group a sequence of words surrounding a noun into a noun phrase such as \emph{the Imperial students}.
Words grouped into constituents can be used as an intermediate form for semantic analysis and checking whether a sentence is grammatically correct.
In the context of this project, it will be utilised as a part of atomic sentence extraction.


The resulting structure produced by constituency parsing is a parse tree. 
An example of a tree parsed by a constituency parser can be seen in \ref{constituency-graph}.

\begin{figure}[h]
% https://parser.kitaev.io/
% INSERT https://spacy.io/universe/project/self-attentive-parser
% TODO find where labels are defined.
\caption{Parse tree of \emph{The fielder caught the ball.} as generated by spacy Berkley Neural Parser.}
\centering
\includegraphics[width=0.8\textwidth]{background/constituency parse.png}
\label{constituency-graph}
\end{figure}

% How is it done if need more content?

% \section{Other Relevant Machine Learning Topics}
% 
% TODO
% 
% \subsection{Video Processing}
% 
% \subsubsection{Convolutional Neural Networks}
% 
% % Check those from their papers.
% 
% % Check more stuff from computer vision.
% 
% \subsubsection{Feature Extractors}
% 
% \subsection{Attention}






\chapter{Related Work}

\section{Inherited Work}
\label{inherited-work}

This thesis has not been started from scratch.

% TODO reference paper %
The project itself is a continuation of the \emph{Automatic Concept Extraction for Concept Bottleneck-based Video Classification} \cite{RefWorks:RefID:16-2021automatic} which my supervisors authored.
This paper is currently under review for the \href{https://iclr.cc/}{ICLR 2022} conference. \\

In this work, the following has been completed:

 Concept Discovery and Extraction Module (CoDEx) is proposed, which automatically uses video explanations to extract concepts.
 
 - Model has been evaluated to validate it obtains competitive performance with standard end-to-end models.
 
 - The concept-based architecture has been augmented with the attention mechanism, which enables verifying how important each concept is for a decision.
 
 - Two public datasets are constructed, MLB-V2E and MSR-V2E. Both combine crowd-sourced explanations with short video sequences. The difference is that the MSR-V2E focuses mainly on general videos while MLB-V2E is baseball-related.
\\

The CoDEx is a vital element of the project.
The presented Concept Discovery and Extraction Module in the paper consists of 6 stages: cleaning, extraction, grouping, completion, pruning, and vectorisation.
The cleaning stage removes videos/explanations which are corrupted.
The extraction step uses a constituency parser and a rule-based approach to determine whether a part of the sentence  should be included as a candidate concept. 
The rules are shown in the following table.

\begin{center}
\begin{tabular}{ |p{2cm}|p{12cm}|  }
 \hline
 \multicolumn{2}{|c|}{Rules determining whether a candidate concept should be included or excluded} \\
 \hline
 rule name & rule \\
 \hline
 Inclusion 1 & noun/pronoun → auxillary (optional) → particle (optional) → verb (optional) \\
 Inclusion 2 & noun/pronoun → auxiliary whose lemma is 'be' → any token \\
 Exclusion & subordinating conjunction \\
 
 \hline
 
\end{tabular}
\label{inclusion-exclusion-rules}
\end{center}

The completion stage looks up for concepts identified in some explanations while not other ones due to the behaviour of the constituency parser.
That stage is done by the substring lookup of existing concepts in all sentences where previous steps did not find that concept.

The grouping step attempts to group concepts with similar meanings using agglomerative clustering \cite{RefWorks:RefID:13-mullner2011modern}. 
It also removes concepts that have a small frequency.

The pruning stage attempts to keep highly informative concepts by picking the smallest concept subset such that the mutual information \cite{RefWorks:RefID:30-mackay2004information} between the label and a concept vector does not fall below a certain percentage $\gamma$.
This problem is not solved precisely, but rather concepts are inserted using a greedy approach until the mutual information between the label and the newly constructed vector is not below a percentage $\gamma$.


The vectorisation constructs the N x K concept matrix, where K is the number of concepts and N is the number of data points. 
The cell (n, k) is one if the concept k occurs for the data point n, zero otherwise.

\section{Video Classification}

Even though the work on video classification is not closely related to this thesis's contributions, it should serve as a performance benchmark of the entire system.

There are a few popular datasets for video classification out there, such as YouTube-8M \cite{RefWorks:RefID:10-abu-el-haija2016youtube-8m:} and Kinetics \cite{RefWorks:RefID:9-carreira2017quo}.
The YouTube-8M \cite{RefWorks:RefID:10-abu-el-haija2016youtube-8m:} is an extensive benchmark for general video classification with around 8 million clips and 4800 visual entities.
Mao et al. \cite{RefWorks:RefID:4-mao2019hierarchical} achieved very high performance on the YouTube-8M dataset using a deep convolutional graph neural network and a multi-level feature extractor.
Kinetics-700 \cite{RefWorks:RefID:14-carreira2019short} dataset is the latest version of the popular Kinetics dataset, which contains 700 human action classes and around 650000 video clips.
The top-performing model at the dataset has been designed by Yan et al. which uses Multiview Transformers for Video Recognition. 
The core idea of this approach is to input multiple different "views" of an input video into a transformer model to achieve better results.
This approach resulted in a model with an accuracy of 82.2\% on the Kinetics-700 dataset with the top-5 accuracy of 95.7\%.

YouTube-8M and Kinetics datasets are somewhat different to the dataset the majority of the project will address.
The MLB-V2E sequences look very similar, given that all of the sequences start with a pitcher throwing a ball.

A dataset much more closely related to our problem is the MLB-YouTube \cite{RefWorks:RefID:3-piergiovanni2018fine-grained} dataset.
The segmented video classification part of this dataset is the base for the MLB-V2E \cite{RefWorks:RefID:16-2021automatic} dataset which also includes crowd-sourced explanations. 
The MLB-YouTube dataset is quite different from the alternatives discussed in this section because the scene is very similar in all videos, only a single camera view is used, and the activity itself is only different because of the movement of one person.
The MLB-YouTube paper validates the InceptionV3 \cite{RefWorks:RefID:49-szegedy2016rethinking} and I3D \cite{RefWorks:RefID:9-carreira2017quo} networks performance on the constructed datasets shown in the table below.
These results should serve as a guide for the results of this project.

\begin{center}
\begin{tabular}{ |p{2cm}|p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm}|  }
 \hline
 \multicolumn{9}{|c|}{Per-class average precision for the multi-class baseball classification} \\
 \hline
 Method & Ball & Strike & Swing & Hit & Foul & In Play & Bunt & Hit by Pitch \\
 \hline
 Random & 21.8 & 28.6 & 37.4 & 20.9 & 11.4 & 10.3 & 1.1 & 4.5 \\
 InceptionV3 & 66.9 & 93.9 & 90.3 & 90.9 & 60.7 & 89.7 & 12.4 & 29.2 \\
 I3D & 62.5 & 91.3 & 88.5 & 86.5 & 47.3 & 75.9 & 16.2 & 21.0 \\
 \hline
 
\end{tabular}
\label{inclusion-exclusion-rules}
\end{center}


\section{Definition of Concept in Other Settings}

There are different angles to the definition of concepts in various works.
As mentioned, this project defines a concept as a syntactic generalisation of an atomic sentence.
However, this approach is not common in other works which do concept extraction.


Formal concept analysis \cite{RefWorks:RefID:31-ganter2012formal} is a method for knowledge representation based on the lattice theory which can be used to extract hierarchical concepts.
It defines two fundamental notions: a formal context and a formal concept. 
Formal context K := (G, M, I) consists of a set of objects G, set of attributes M and relation I on (G, M).
If (g, m) $\in$ I, object g has attribute m.
Using these attributes, set A$' $ is defined containing all attributes common to the objects in a set of objects A.
Additionally, set B$' $ contains objects that have all attributes in the set of relations B.
From these definitions, formal concept of context (G, M, I) is defined as a pair (A, B) such that A $\subseteq$ G, B $\subseteq$ M, B$' $ = A and A$' $ = B.

The Formal Concept Analysis with text analysis is mainly used to construct the concept hierarchy, where a concept refers to a phrase containing two words.

For example, work by Cimiano et al. \cite{RefWorks:RefID:32-cimiano2005learning} extracts verb/subject, verb/object/ and verb/prepositional phrase as candidate concepts.
Moreover, the Formal Concept Analysis is used by Anoop et al. \cite{RefWorks:RefID:33-anoop2019extracting} to extract concepts with their relationships from unstructured text.
The authors manually extract stemmed noun phrases as key phrases and use indications such as "is-a" to generate a formal context table.
This table is then used to extract hierarchical concepts.

On the other hand, TaxoLearn \cite{RefWorks:RefID:34-dietz2012taxolearn} by Dietz et al. does not use Formal Concept Analysis but also extracts noun phrases as concepts. The relevance of noun phrases is checked by computing how often they appear in a particular context compared to other contexts. It also uses a hierarchical clustering algorithm to construct a taxonomy.

Moreover, approaches such as the one by Koh et al. \cite{RefWorks:RefID:35-koh2020concept} define concepts as properties that the image may have. These properties need to be provided beforehand.

Finally, work such as the one by Fan et al. \cite{RefWorks:RefID:50-fan2004semantic} use expert defined terms, such as \emph{Lecture Presentation for Gastrointestinal Surgery}, as semantic concepts. 


\section{Concept-Based Explanations for Images and Text}

Concept-based explanations for images and text attempts is a more straightforward related problem.
Koh et al. \cite{RefWorks:RefID:35-koh2020concept} predict a set of pre-labelled concepts which they use to make a final classification.
The pipeline by Koh et al. is similar to the pipeline that will be used in this project, just that the concepts are automatically extracted instead of provided.
Yeh et al. \cite{RefWorks:RefID:36-yeh2019completeness-aware} propose a method that automatically extracts sets of pixels from an image that represent valuable concepts.
Similarly, Ghorbani et al. \cite{RefWorks:RefID:37-ghorbani2019automatic} propose another method that automatically extracts a set of visual concepts which are meaningful to humans.
All outlined methods are based on visual concept extraction, unlike this project's concept extraction, which will mainly focus on the events/actions in the baseball video sequence.


\section{Video Explanation}

Another related problem is that of Video Explanations. This project may even explore it at a later stage within the context of a concept extraction pipeline.

One famous dataset for video understanding is the MSR-VTT dataset \cite{RefWorks:RefID:40-jun2016msr-vtt:}. It is a large scale dataset with more than 10000 video clips in 257 popular categories of a video search engine.
Each video is annotated with roughly 20 sentences. 
A few different approaches for tackling this issue are presented as viable options in the paper, such as 2D Convolutions, 3D Convolutions, and RNNs.
One of the most performant methods on this dataset is CLIP2TV \cite{RefWorks:RefID:41-gao2021clip2tv:}, which utilises transformer-based techniques for both video and text representation.


The authors of the MLB-V2E \cite{RefWorks:RefID:16-2021automatic} have also constructed the MSR-V2E \cite{RefWorks:RefID:16-2021automatic} dataset, which uses the clips made available by the MSR-VTT and provides new classification labels and explanations.
This dataset will likely be used for the evaluation of this project.

\section{Semantic Concept Video Classification}

Semantic Concept Video Classification attempts to predict a set of predefined concepts from a video.
Predicting a set of predefined concepts from a video is closely related to a part of the pipeline in this project, which indicates which extracted concepts occur before predicting the outcome. 

The work by Fan et al. \cite{RefWorks:RefID:50-fan2004semantic} tries to predict semantically defined concepts by medical experts.
The paper proposes using salient objects, which are visually-distinguishable video components that human semantics understands, to model these semantical concepts.
Examples of notions salient objects attempt to represent are a face, voice, or a lecture slide.
Despite its benefits, the semantics of salient objects is quite simple, and it does not model relationships that happen over multiple frames in a video.
Newer work by Fan et al. \cite{RefWorks:RefID:51-jianping2007incorporating} also incorporates the possibility of a hierarchical concept classification. The approach also predicts atomic video concepts before constructing the higher-level ones and existing concept ontology.

Assari et al. \cite{RefWorks:RefID:52-assari2014video} represent a video by the co-occurrence of the semantic concepts before applying a classifier.
The concepts in this work are also predefined, but they represent a set of events rather than a set of objects.



